
@Article{Cheng2022,
AUTHOR = {Cheng, Xiuquan and Zhang, Shaobo and Cheng, Sizhu and Xia, Qinxiang and Zhang, Junhao},
TITLE = {Path-Following and Obstacle Avoidance Control of Nonholonomic Wheeled Mobile Robot Based on Deep Reinforcement Learning},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {14},
ARTICLE-NUMBER = {6874},
URL = {https://www.mdpi.com/2076-3417/12/14/6874},
ISSN = {2076-3417},
ABSTRACT = {In this paper, a novel path-following and obstacle avoidance control method is given for nonholonomic wheeled mobile robots (NWMRs), based on deep reinforcement learning. The model for path-following is investigated first, and then applied to the proposed reinforcement learning control strategy. The proposed control method can achieve path-following control through interacting with the environment of the set path. The path-following control method is mainly based on the design of the state and reward function in the training of the reinforcement learning. For extra obstacle avoidance problems in following, the state and reward function is redesigned by utilizing both distance and directional perspective aspects, and a minimum representative value is proposed to deal with the occurrence of multiple obstacles in the path-following environment. Through the reinforcement learning algorithm deep deterministic policy gradient (DDPG), the NWMR can gradually achieve the path it is required to follow and avoid the obstacles in simulation experiments, and the effectiveness of the proposed algorithm is verified.},
DOI = {10.3390/app12146874}
}

@article{lilli2015,
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous
action domain. We present an actor-critic, model-free algorithm based on the deterministic
policy gradient that can operate over continuous action spaces. Using
the same learning algorithm, network architecture and hyper-parameters, our algorithm
robustly solves more than 20 simulated physics tasks, including classic
problems such as cartpole swing-up, dexterous manipulation, legged locomotion
and car driving. Our algorithm is able to find policies whose performance is competitive
with those found by a planning algorithm with full access to the dynamics
of the domain and its derivatives. We further demonstrate that for many of the
tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
  added-at = {2019-08-19T09:40:21.000+0200},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  biburl = {https://www.bibsonomy.org/bibtex/22708c349821330660afb992aec2be5d1/tobias.koopmann},
  booktitle = {ICLR},
  crossref = {conf/iclr/2016},
  editor = {Bengio, Yoshua and LeCun, Yann},
  ee = {http://arxiv.org/abs/1509.02971},
  interhash = {b791167abe535c8525f6a9bf62fcc1ab},
  intrahash = {2708c349821330660afb992aec2be5d1},
  keywords = {readinggroup},
  timestamp = {2019-09-04T10:51:45.000+0200},
  title = {Continuous control with deep reinforcement learning.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2016.html#LillicrapHPHETS15},
  venue = {ICLR},
  year = 2016
}

@article{schulman2017,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{duguleana2016,
  title={Neural networks based reinforcement learning for mobile robots obstacle avoidance},
  author={Duguleana, Mihai and Mogan, Gheorghe},
  journal={Expert Systems with Applications},
  volume={62},
  pages={104--115},
  year={2016},
  publisher={Elsevier}
}

@article{watkins1989,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  publisher={King's College, Cambridge United Kingdom}
}


@article{mnih2013,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{feng2019,
  title={Mobile robot obstacle avoidance based on deep reinforcement learning},
  author={Feng, Shumin and Ren, Hailin and Wang, Xinran and Ben-Tzvi, Pinhas},
  booktitle={International Design Engineering Technical Conferences and Computers and Information in Engineering Conference},
  volume={59230},
  pages={V05AT07A048},
  year={2019},
  organization={American Society of Mechanical Engineers}
}

@article{aznar2019,
  title={Obtaining fault tolerance avoidance behavior using deep reinforcement learning},
  author={Aznar, Fidel and Pujol, Mar and Rizo, Ram{\'o}n},
  journal={Neurocomputing},
  volume={345},
  pages={77--91},
  year={2019},
  publisher={Elsevier}
}


@article{choi2019,
  title={Deep reinforcement learning of navigation in a complex and crowded environment with a limited field of view},
  author={Choi, Jinyoung and Park, Kyungsik and Kim, Minsu and Seok, Sangok},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={5993--6000},
  year={2019},
  organization={IEEE}
}



@article{leiva2020,
  title={Robust rl-based map-less local planning: Using 2d point clouds as observations},
  author={Leiva, Francisco and Ruiz-del-Solar, Javier},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={4},
  pages={5787--5794},
  year={2020},
  publisher={IEEE}
}

@article{mnih2016,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@article{mirowski2016,
  title={Learning to navigate in complex environments},
  author={Mirowski, Piotr and Pascanu, Razvan and Viola, Fabio and Soyer, Hubert and Ballard, Andrew J and Banino, Andrea and Denil, Misha and Goroshin, Ross and Sifre, Laurent and Kavukcuoglu, Koray and others},
  journal={arXiv preprint arXiv:1611.03673},
  year={2016}
}

@article{zeng2020,
  title={Visual navigation with asynchronous proximal policy optimization in artificial agents},
  author={Zeng, Fanyu and Wang, Chen},
  journal={Journal of Robotics},
  volume={2020},
  pages={1--7},
  year={2020},
  publisher={Hindawi Limited}
}
@article{zhu2017,
  title={Target-driven visual navigation in indoor scenes using deep reinforcement learning},
  author={Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3357--3364},
  year={2017},
  organization={IEEE}
}

@article{devo2020,
  title={Towards generalization in target-driven visual navigation by using deep reinforcement learning},
  author={Devo, Alessandro and Mezzetti, Giacomo and Costante, Gabriele and Fravolini, Mario L and Valigi, Paolo},
  journal={IEEE Transactions on Robotics},
  volume={36},
  number={5},
  pages={1546--1561},
  year={2020},
  publisher={IEEE}
}




